{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_1_Layers_and_Blocks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTfVLDo612ytMkOQIMHHWI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VirtueZhao/Dive_into_Deep_Learning/blob/main/5_1_Layers_and_Blocks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhU5p-Df3ZyB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e51a82b-a7d3-4871-8513-a7dc203c641b"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "net = nn.Sequential(nn.Linear(20,256),nn.ReLU(), nn.Linear(256,10))\n",
        "X = torch.rand(2,20)\n",
        "net(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1656, -0.0791, -0.1423, -0.1286, -0.2268,  0.0812,  0.1525, -0.0625,\n",
              "         -0.0389,  0.2103],\n",
              "        [ 0.2113, -0.1490, -0.2432, -0.1408, -0.2733,  0.1499,  0.1506, -0.0350,\n",
              "         -0.0520,  0.2659]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQh6QKkIJN8K"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.hidden = nn.Linear(20, 256)\n",
        "    self.out = nn.Linear(256, 10)\n",
        "\n",
        "  def forward(self, X):\n",
        "    return self.out(F.relu(self.hidden(X)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C42ILgEKgTO",
        "outputId": "6c35acb8-a0ed-4f77-a6ba-f15642bd6ce4"
      },
      "source": [
        "net = MLP()\n",
        "net(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0107,  0.0127, -0.1246,  0.0762, -0.0168,  0.2129, -0.2542, -0.0209,\n",
              "         -0.0710, -0.4358],\n",
              "        [ 0.1027,  0.1595, -0.1034, -0.0147, -0.0995,  0.1580, -0.2705, -0.0193,\n",
              "         -0.1729, -0.4051]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7csDw-LLUwi"
      },
      "source": [
        "class MySequential(nn.Module):\n",
        "  def __init__(self, *args):\n",
        "    super().__init__()\n",
        "    for block in args:\n",
        "      print(\"block: \", block)\n",
        "      self._modules[block] = block\n",
        "\n",
        "  def forward(self, X):\n",
        "    for block in self._modules.values():\n",
        "      print(\"block: \", block)\n",
        "      print(\"X.shape: \", X.shape)\n",
        "      X = block(X)\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "donLmRZ9MgrZ",
        "outputId": "639b006d-d985-4171-b5b0-8cbb9bdf66f2"
      },
      "source": [
        "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "block:  Linear(in_features=20, out_features=256, bias=True)\n",
            "block:  ReLU()\n",
            "block:  Linear(in_features=256, out_features=10, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_ROtKqhMtvp",
        "outputId": "87e6b03a-4c7b-460f-962b-f806770ea1d1"
      },
      "source": [
        "net(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "block:  Linear(in_features=20, out_features=256, bias=True)\n",
            "X.shape:  torch.Size([2, 20])\n",
            "block:  ReLU()\n",
            "X.shape:  torch.Size([2, 256])\n",
            "block:  Linear(in_features=256, out_features=10, bias=True)\n",
            "X.shape:  torch.Size([2, 256])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-6.6642e-02, -1.2212e-02, -2.8864e-01,  1.1474e-01,  6.2709e-02,\n",
              "         -6.5087e-02, -1.5789e-01,  1.2593e-01,  6.4001e-02, -2.1273e-04],\n",
              "        [ 1.1726e-01, -4.8543e-02, -3.1946e-01,  3.3828e-02,  1.6683e-01,\n",
              "          2.1858e-02, -2.2797e-01,  5.9341e-02,  8.4259e-03,  8.0541e-02]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xAlkbAeNE5A"
      },
      "source": [
        "class FixedHiddenMLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.rand_weight = torch.rand((20,20), requires_grad=False)\n",
        "    # print(\"Random Constant Weight: \", self.rand_weight)\n",
        "    self.linear = nn.Linear(20, 20)\n",
        "  \n",
        "  def forward(self, X):\n",
        "    X = self.linear(X)\n",
        "    X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
        "    X = self.linear(X)\n",
        "\n",
        "    while X.abs().sum() > 1:\n",
        "      # print(\"X L1 Norm: \", X.abs().sum())\n",
        "      X /= 2\n",
        "    return X.sum()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu0g9SM0Ofp-",
        "outputId": "f97937b1-e402-4239-9017-47c5f3a37de7"
      },
      "source": [
        "net = FixedHiddenMLP()\n",
        "net(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Constant Weight:  tensor([[0.7780, 0.6669, 0.0199, 0.3668, 0.0355, 0.4385, 0.8643, 0.5396, 0.2862,\n",
            "         0.3566, 0.3343, 0.8176, 0.6625, 0.2531, 0.5812, 0.1201, 0.8012, 0.3972,\n",
            "         0.3455, 0.5672],\n",
            "        [0.0453, 0.8185, 0.3087, 0.4420, 0.3341, 0.8902, 0.0781, 0.4348, 0.6033,\n",
            "         0.8551, 0.0284, 0.7537, 0.0423, 0.3991, 0.4727, 0.7780, 0.8208, 0.3093,\n",
            "         0.2980, 0.6577],\n",
            "        [0.5202, 0.9986, 0.2730, 0.0446, 0.2660, 0.9463, 0.2272, 0.5762, 0.5771,\n",
            "         0.1202, 0.7382, 0.6326, 0.2167, 0.2782, 0.7541, 0.2141, 0.6906, 0.8236,\n",
            "         0.7871, 0.1336],\n",
            "        [0.9095, 0.9238, 0.3386, 0.9166, 0.0755, 0.7112, 0.0574, 0.2409, 0.4076,\n",
            "         0.2596, 0.2366, 0.1707, 0.2993, 0.8163, 0.5489, 0.6811, 0.5454, 0.2092,\n",
            "         0.9712, 0.6705],\n",
            "        [0.5310, 0.9534, 0.9895, 0.0912, 0.3847, 0.9271, 0.1175, 0.2558, 0.1656,\n",
            "         0.4235, 0.5459, 0.7059, 0.4238, 0.9901, 0.3741, 0.8011, 0.4908, 0.3588,\n",
            "         0.7747, 0.4349],\n",
            "        [0.4497, 0.3724, 0.0294, 0.8267, 0.9162, 0.4575, 0.5864, 0.2812, 0.3700,\n",
            "         0.7272, 0.1839, 0.4697, 0.5968, 0.3871, 0.1267, 0.4162, 0.3550, 0.3926,\n",
            "         0.3894, 0.9448],\n",
            "        [0.4407, 0.4118, 0.7893, 0.7562, 0.2194, 0.4539, 0.9354, 0.7094, 0.5315,\n",
            "         0.2614, 0.8450, 0.6933, 0.7450, 0.6668, 0.3077, 0.4621, 0.8142, 0.3504,\n",
            "         0.1327, 0.8625],\n",
            "        [0.6954, 0.6006, 0.1607, 0.1929, 0.3061, 0.0641, 0.6743, 0.4667, 0.6165,\n",
            "         0.6024, 0.7584, 0.5588, 0.0935, 0.3698, 0.4548, 0.3150, 0.9133, 0.3554,\n",
            "         0.0857, 0.8053],\n",
            "        [0.6061, 0.0339, 0.8008, 0.5017, 0.4215, 0.7515, 0.1296, 0.4896, 0.9703,\n",
            "         0.4849, 0.8697, 0.5004, 0.3799, 0.4192, 0.4635, 0.3193, 0.8561, 0.0585,\n",
            "         0.9938, 0.0127],\n",
            "        [0.8649, 0.0248, 0.1643, 0.9386, 0.8125, 0.9591, 0.1760, 0.2514, 0.4247,\n",
            "         0.6197, 0.4673, 0.3953, 0.3505, 0.9314, 0.2327, 0.6642, 0.8495, 0.4369,\n",
            "         0.8233, 0.0357],\n",
            "        [0.7402, 0.2612, 0.3113, 0.4271, 0.2797, 0.5901, 0.8087, 0.8948, 0.0561,\n",
            "         0.7634, 0.4304, 0.9124, 0.1184, 0.0014, 0.9259, 0.3338, 0.6074, 0.4112,\n",
            "         0.7907, 0.1677],\n",
            "        [0.4248, 0.6431, 0.6057, 0.7862, 0.6597, 0.3442, 0.4855, 0.9607, 0.6259,\n",
            "         0.5390, 0.1958, 0.9318, 0.5596, 0.8924, 0.0219, 0.9624, 0.1531, 0.5660,\n",
            "         0.1634, 0.5350],\n",
            "        [0.6195, 0.4260, 0.8966, 0.1871, 0.6550, 0.5378, 0.8284, 0.1108, 0.7452,\n",
            "         0.8512, 0.1977, 0.3917, 0.7771, 0.9588, 0.3975, 0.2513, 0.5996, 0.4717,\n",
            "         0.3987, 0.5703],\n",
            "        [0.6177, 0.7961, 0.8957, 0.0062, 0.6104, 0.5058, 0.4675, 0.3918, 0.5252,\n",
            "         0.4797, 0.6051, 0.4210, 0.6978, 0.7076, 0.6593, 0.4676, 0.1007, 0.2758,\n",
            "         0.8624, 0.7723],\n",
            "        [0.1949, 0.0612, 0.3251, 0.5125, 0.5582, 0.9948, 0.5874, 0.1926, 0.0964,\n",
            "         0.8836, 0.7332, 0.7472, 0.8168, 0.3996, 0.3897, 0.3373, 0.9961, 0.2092,\n",
            "         0.3037, 0.9050],\n",
            "        [0.0664, 0.8488, 0.2729, 0.8887, 0.7974, 0.7879, 0.8589, 0.6997, 0.5569,\n",
            "         0.6860, 0.0629, 0.6375, 0.4066, 0.4203, 0.8668, 0.3295, 0.3862, 0.2814,\n",
            "         0.0963, 0.6432],\n",
            "        [0.3192, 0.5464, 0.3663, 0.2019, 0.5703, 0.6607, 0.9742, 0.4238, 0.4332,\n",
            "         0.9261, 0.2586, 0.1470, 0.6634, 0.1446, 0.4245, 0.4782, 0.0578, 0.0737,\n",
            "         0.4076, 0.7752],\n",
            "        [0.5802, 0.7295, 0.8617, 0.8312, 0.9262, 0.1488, 0.1060, 0.6575, 0.0124,\n",
            "         0.3237, 0.4830, 0.4296, 0.6012, 0.6516, 0.3442, 0.3048, 0.7541, 0.4307,\n",
            "         0.9851, 0.2930],\n",
            "        [0.5459, 0.3313, 0.2812, 0.5195, 0.9768, 0.6042, 0.6077, 0.7922, 0.7376,\n",
            "         0.1916, 0.0777, 0.4322, 0.2047, 0.0384, 0.1906, 0.1281, 0.5128, 0.6226,\n",
            "         0.2115, 0.7204],\n",
            "        [0.0490, 0.6598, 0.9456, 0.1266, 0.9121, 0.3526, 0.9258, 0.2623, 0.9083,\n",
            "         0.8891, 0.0193, 0.0043, 0.2292, 0.5238, 0.7823, 0.7098, 0.0710, 0.1502,\n",
            "         0.6359, 0.5862]])\n",
            "X L1 Norm:  tensor(33.7685, grad_fn=<SumBackward0>)\n",
            "X L1 Norm:  tensor(16.8843, grad_fn=<SumBackward0>)\n",
            "X L1 Norm:  tensor(8.4421, grad_fn=<SumBackward0>)\n",
            "X L1 Norm:  tensor(4.2211, grad_fn=<SumBackward0>)\n",
            "X L1 Norm:  tensor(2.1105, grad_fn=<SumBackward0>)\n",
            "X L1 Norm:  tensor(1.0553, grad_fn=<SumBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.2990, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE4xXXesPBD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d829dee8-4773-4d7d-d50e-3af70434b6e3"
      },
      "source": [
        "class NestMLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(20,64), nn.ReLU(),\n",
        "                  nn.Linear(64,32), nn.ReLU())\n",
        "    self.linear = nn.Linear(32, 16)\n",
        "  \n",
        "  def forward(self, X):\n",
        "    return self.linear(self.net(X))\n",
        "\n",
        "chimera = nn.Sequential(NestMLP(), nn.Linear(16,20), FixedHiddenMLP())\n",
        "chimera(X)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-0.1993, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}
